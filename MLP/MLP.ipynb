{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fed366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 09:44:33.631431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 09:44:33.908296: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 09:44:33.908592: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 09:44:35.671632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 09:44:35.671891: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 09:44:35.671897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pyconll, keras, pickle, os, random, nltk, datetime, warnings, gc, urllib.request, zipfile\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.sparse import hstack, vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, BatchNormalization, Dropout, Input, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "# from sklearn.metrics.classification import UndefinedMetricWarning\n",
    "# warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1297ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(\"/home/chitrang/Documents/CSE-582/\") # setting the root dir\n",
    "POS_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "pos_train = os.path.join(POS_DIR, \"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74586842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(fname, include_y=True):\n",
    "    sentences = [] # master list\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    sentence = [] # local list\n",
    "    for line in content:\n",
    "        if line !='\\n':\n",
    "            line = line.strip() # remove leading/trailing spaces\n",
    "            word = line.split()[0].lower() # get the word\n",
    "            if include_y:\n",
    "                pos = \"\"\n",
    "                pos = line.split()[1] # get the pos tag\n",
    "                sentence.append((word, pos)) # create a pair and save to local list\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "        else:\n",
    "            sentences.append(sentence) # once a \\n is detected, append the local sentence to master sentence\n",
    "            sentence = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b01094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = format_data(pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517a6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences in train set:  8936\n",
      "Tagged words in train set: 211727\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08f7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset Split\n",
    "# total_train = len(train_sentences)\n",
    "# print(total_train)\n",
    "# val_split = math.floor(0.2 * total_train)\n",
    "# print(val_split)\n",
    "# train_sentences, val_sentences = train_sentences[:(total_train-val_split)], train_sentences[(total_train-val_split):]\n",
    "# print(len(train_sentences), len(val_sentences))\n",
    "# print(len(train_sentences)+len(val_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b72eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3bf2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sequence(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def text_sequence(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def id2word(sentences):\n",
    "    wordlist = [item for sublist in text_sequence(sentences) for item in sublist]\n",
    "    id2word = {k:v for k,v in enumerate(wordlist)}\n",
    "    return id2word\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def untag_pos(tagged_sentence):\n",
    "    return [t for _, t in tagged_sentence]\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab =set()\n",
    "    for sentence in sentences:\n",
    "        for word in untag(sentence):\n",
    "            vocab.add(word)\n",
    "    return sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6061bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = build_vocab(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b4b2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56b702a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_path = '/home/chitrang/Downloads/wiki-news-300d-1M.vec'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embs_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa731631",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c = dict()\n",
    "for item in embeddings.key_to_index:\n",
    "    w2c[item] = embeddings.key_to_index[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e31b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)\n",
    "np.random.seed(3)\n",
    "oov = np.random.uniform(-0.25, 0.25, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6722609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_embs(sentence, index, window=1):\n",
    "    unknown=0\n",
    "    vec = np.array([])\n",
    "    for i in range(index-window, index+window+1):\n",
    "#         if i < 0:\n",
    "#             vec = np.append(vec, pad)\n",
    "#         if i > len(sentence)-1:\n",
    "#             vec = np.append(vec, pad)\n",
    "        try:\n",
    "            vec = np.append(vec, embeddings[sentence[i]])\n",
    "        except:\n",
    "            vec = np.append(vec, oov)\n",
    "            unknown += 1\n",
    "    return vec, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a597ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_basic(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'nb_terms': len(sentence),        \n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'i-1_prefix-3': '' if index == 0 else sentence[index-1][:3],        \n",
    "        'i-1_suffix-3': '' if index == 0 else sentence[index-1][-3:],        \n",
    "        'i+1_prefix-3': '' if index == len(sentence) - 1 else sentence[index+1][:3],        \n",
    "        'i+1_suffix-3': '' if index == len(sentence) - 1 else sentence[index+1][-3:],        \n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21bfa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences, window):\n",
    "    i=0\n",
    "    X, y = [], []\n",
    "    for doc_index, tagged in enumerate(tagged_sentences):\n",
    "        for index in range(len(tagged)):\n",
    "            X.append([features_basic(untag(tagged), index),\\\n",
    "                      features_embs(untag(tagged), index, window)[0],\\\n",
    "                     ])\n",
    "            y.append(tagged[index][1])\n",
    "            #features_embs(untag(tagged), index, window)[1]\n",
    "            k = features_embs(untag(tagged), index, window)[1]\n",
    "            i += k\n",
    "    return X, y, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e526dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_sentence(sentence, window):\n",
    "    X = []\n",
    "    for index in range(len(sentence)):\n",
    "            X.append([\n",
    "                      features_basic(sentence, index),\\\n",
    "                      features_embs(sentence, index, window),\\\n",
    "                     ])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4b5c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset_unknown(sentences, window):\n",
    "    X = []\n",
    "    for doc_index, sentence in enumerate(sentences):\n",
    "        for index in range(len(sentence)):\n",
    "            X.append([features_basic(sentence, index),\\\n",
    "                      features_embs(sentence, index, window),\\\n",
    "                     ])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a94c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(features_embs(train_sentences[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd9f08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(untag_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fc31339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untag_sentence_vector = []\n",
    "# for i, word in enumerate(untag_train):\n",
    "#     print(len(features_embs(untag_train, i, window=1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a758e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1faca4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_list = []\n",
    "# temp_list.append(train_sentences[0])\n",
    "# temp_list.append(train_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0699e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 sentences, first one with 37 words, second one with 27 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34baa01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, i = transform_to_dataset(temp_list, 1)\n",
    "# # transformed = transform_to_dataset(temp_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09917678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed[0] -> untagged part, transformed[1] -> pos_tags\n",
    "# transformed[0][0] -> each sentence\n",
    "# transformed[0][0][0] -> features basic list\n",
    "# transformed[0][0][1] -> features embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2a9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(transformed[1]) -> 64 i.e 37+27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4a11a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = transformed[0][0] -> for each sentence, for each word it has features basic, features embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a10d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = transformed[1] -> all pos tags collected in a single list for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c38bc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "# classical\n",
    "def vectorize(train, window=1):\n",
    "    print('Embeddings window method')\n",
    "    print('Vectorizing Dataset...')\n",
    "    X_train, y_train, unk_tr = transform_to_dataset(train, window=window)\n",
    "    X_train = [x[1] for x in X_train]\n",
    "    X_train = np.asarray(X_train)\n",
    "#     print('Vectorizing train...')\n",
    "#     X_train, y_train, _ = transform_to_dataset(train, window=window)\n",
    "# #     hasher = FeatureHasher(n_features=2**20)\n",
    "# #     X_train = hasher.transform([x[0] for x in X_train])\n",
    "#     v = DictVectorizer(sparse=True) \n",
    "#     X_train = v.fit_transform([x[0] for x in X_train])\n",
    "    \n",
    "# #     print('Vectorizing val...')\n",
    "# #     X_val, y_val, _ = transform_to_dataset(val, window=window)\n",
    "# #     X_val = v.fit_transform([x[0] for x in X_val])\n",
    "# #     print('Dataset vectorized.')\n",
    "# #     print('Train shape:', X_train.shape)\n",
    "# #     print('Val shape:', X_val.shape)\n",
    "#     X_train = X_train\n",
    "#     X_val = X_val\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67ef670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher, DictVectorizer\n",
    "def vectorize_unknown(test, window=1):\n",
    "    print('Vectorizing test...')\n",
    "    X_test = transform_to_dataset_unknown(test, window=window)\n",
    "#     hasher = FeatureHasher(n_features=2**20)\n",
    "#     X_train = hasher.transform([x[0] for x in X_train])\n",
    "    v = DictVectorizer(sparse=True) \n",
    "    X_test = v.fit_transform([x[0] for x in X_test])\n",
    "    X_test = X_test\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5fc6484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings window method\n",
      "Vectorizing Dataset...\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = vectorize(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b21e1b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211727, 900)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1929a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48b83d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211727, 900)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce5df202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3039440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211727, 44)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "# y_val = le.transform(y_val)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "# y_val = keras.utils.to_categorical(y_val)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b442450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # In the first layer, we specify the input data shape\n",
    "\n",
    "# model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "419d05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_epoch = 10\n",
    "# batch_size = 128\n",
    "# early_stopping = EarlyStopping(monitor = 'val_acc', patience = 5)\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     epochs=nb_epoch,\n",
    "#                     batch_size=batch_size,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     verbose=1,\n",
    "#                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f65847b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train = vstack((X_train, X_val)).tocsr()\n",
    "y_full_train = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa039ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169381, 70531)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "100c9795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 200)               180200    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                12864     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 44)                2860      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 195,924\n",
      "Trainable params: 195,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Dense(200) is a fully-connected layer with 200 hidden units.\n",
    "# In the first layer, we specify the input data shape\n",
    "\n",
    "model.add(Dense(200, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5ef22568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6617/6617 [==============================] - 20s 3ms/step - loss: 0.5049 - accuracy: 0.8575\n",
      "Epoch 2/20\n",
      "6617/6617 [==============================] - 20s 3ms/step - loss: 0.2671 - accuracy: 0.9200\n",
      "Epoch 3/20\n",
      "6617/6617 [==============================] - 19s 3ms/step - loss: 0.2341 - accuracy: 0.9285\n",
      "Epoch 4/20\n",
      "6617/6617 [==============================] - 20s 3ms/step - loss: 0.2166 - accuracy: 0.9335\n",
      "Epoch 5/20\n",
      "6617/6617 [==============================] - 20s 3ms/step - loss: 0.2018 - accuracy: 0.9378\n",
      "Epoch 6/20\n",
      "6617/6617 [==============================] - 19s 3ms/step - loss: 0.1939 - accuracy: 0.9401\n",
      "Epoch 7/20\n",
      "6617/6617 [==============================] - 19s 3ms/step - loss: 0.1878 - accuracy: 0.9409\n",
      "Epoch 8/20\n",
      "6617/6617 [==============================] - 20s 3ms/step - loss: 0.1819 - accuracy: 0.9428\n",
      "Epoch 9/20\n",
      "6617/6617 [==============================] - 20s 3ms/step - loss: 0.1763 - accuracy: 0.9451\n",
      "Epoch 10/20\n",
      "6617/6617 [==============================] - 18s 3ms/step - loss: 0.1726 - accuracy: 0.9463\n",
      "Epoch 11/20\n",
      "6617/6617 [==============================] - 15s 2ms/step - loss: 0.1698 - accuracy: 0.9474\n",
      "Epoch 12/20\n",
      "6617/6617 [==============================] - 16s 2ms/step - loss: 0.1684 - accuracy: 0.9475\n",
      "Epoch 13/20\n",
      "6617/6617 [==============================] - 16s 2ms/step - loss: 0.1643 - accuracy: 0.9484\n",
      "Epoch 14/20\n",
      "6617/6617 [==============================] - 16s 2ms/step - loss: 0.1596 - accuracy: 0.9498\n",
      "Epoch 15/20\n",
      "6617/6617 [==============================] - 16s 2ms/step - loss: 0.1582 - accuracy: 0.9501\n",
      "Epoch 16/20\n",
      "6617/6617 [==============================] - 17s 3ms/step - loss: 0.1544 - accuracy: 0.9514\n",
      "Epoch 17/20\n",
      "6617/6617 [==============================] - 16s 2ms/step - loss: 0.1542 - accuracy: 0.9522\n",
      "Epoch 18/20\n",
      "6617/6617 [==============================] - 16s 2ms/step - loss: 0.1517 - accuracy: 0.9526\n",
      "Epoch 19/20\n",
      "6617/6617 [==============================] - 16s 2ms/step - loss: 0.1489 - accuracy: 0.9530\n",
      "Epoch 20/20\n",
      "6617/6617 [==============================] - 16s 2ms/step - loss: 0.1473 - accuracy: 0.9537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c80a90e80>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=1,\n",
    "                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "985551b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5e4fff9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211727, 900)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ec87d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211727, 44)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d7349e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a6d2ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = os.path.join(POS_DIR, \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ccca5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = format_data(pos_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c4cb144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_ = vectorize_unknown(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8a66245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37516587",
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_test = transform_test_sentence(test_sentences[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "802fbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc3b2d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lets_test[1][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d7c7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = DictVectorizer(sparse=True) \n",
    "# X_embs = v.fit_transform([x[0] for x in lets_test])\n",
    "# X_embs = X_embs\n",
    "# lets_test = X_embs\n",
    "X_embs = [x[1][0] for x in lets_test]\n",
    "X_embs = np.asarray(X_embs)\n",
    "lets_test = X_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fa2d3bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "972f7314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_x=model.predict(lets_test, batch_size=1) \n",
    "classes_x=np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9557b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmaxed = np.argmax(predict_x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a9a838c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 44)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f2e60bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is what our MLP tagger predicts for the test sentence:\n",
      " [('rockwell', 'NNP'), ('international', 'NNP'), ('corp.', 'NNP'), (\"'s\", 'DT'), ('tulsa', 'NNP'), ('unit', 'NNP'), ('said', 'VBD'), ('it', 'PRP'), ('signed', 'VBD'), ('a', 'DT'), ('tentative', 'JJ'), ('agreement', 'NN'), ('extending', 'VBG'), ('its', 'PRP$'), ('contract', 'NN'), ('with', 'IN'), ('boeing', 'NNP'), ('co.', 'NNP'), ('to', 'TO'), ('provide', 'VB'), ('structural', 'JJ'), ('parts', 'NNS'), ('for', 'IN'), ('boeing', 'NNP'), (\"'s\", 'POS'), ('747', 'NN'), ('jetliners', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print('Here is what our MLP tagger predicts for the test sentence:\\n',list(zip(test_sentences[0], le.inverse_transform(argmaxed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "feda2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_correct = '/home/chitrang/Downloads/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8b949bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_test_sen = format_data(pos_test_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6b6b1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sen = zip(test_sentences[0], le.inverse_transform(argmaxed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9268a2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f2c78c015c0>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "534aec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rockwell', 'NNP'),\n",
       " ('international', 'NNP'),\n",
       " ('corp.', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('tulsa', 'NNP'),\n",
       " ('unit', 'NN'),\n",
       " ('said', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('signed', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('tentative', 'JJ'),\n",
       " ('agreement', 'NN'),\n",
       " ('extending', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('contract', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('boeing', 'NNP'),\n",
       " ('co.', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('provide', 'VB'),\n",
       " ('structural', 'JJ'),\n",
       " ('parts', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('boeing', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('747', 'CD'),\n",
       " ('jetliners', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_test_sen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3fd0f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(predicted_sen, correct_sen, acc = True):\n",
    "    total = 0\n",
    "    correct_total = 0\n",
    "    if acc:\n",
    "        for predict, correct in zip(predicted_sen, correct_sen):\n",
    "                if predict[1] == correct[1]:\n",
    "                    correct_total = correct_total + 1\n",
    "                total = total + 1\n",
    "        acc = correct_total / total\n",
    "        return acc\n",
    "    else:\n",
    "        for predict, correct in zip(predicted_sen, correct_sen):\n",
    "                if predict[1] == correct[1]:\n",
    "                    print(f\"{predict[1]} \\t {correct[1]}\")\n",
    "                else:\n",
    "                    print(f\"{predict[1]} \\t {correct[1]} <------ Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c9e2aa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP \t NNP\n",
      "NNP \t NNP\n",
      "NNP \t NNP\n",
      "DT \t POS <------ Error\n",
      "NNP \t NNP\n",
      "NNP \t NN <------ Error\n",
      "VBD \t VBD\n",
      "PRP \t PRP\n",
      "VBD \t VBD\n",
      "DT \t DT\n",
      "JJ \t JJ\n",
      "NN \t NN\n",
      "VBG \t VBG\n",
      "PRP$ \t PRP$\n",
      "NN \t NN\n",
      "IN \t IN\n",
      "NNP \t NNP\n",
      "NNP \t NNP\n",
      "TO \t TO\n",
      "VB \t VB\n",
      "JJ \t JJ\n",
      "NNS \t NNS\n",
      "IN \t IN\n",
      "NNP \t NNP\n",
      "POS \t POS\n",
      "NN \t CD <------ Error\n",
      "NNS \t NNS\n",
      ". \t .\n"
     ]
    }
   ],
   "source": [
    "compare_results(predicted_sen, correct_test_sen[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "154290dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = format_data(pos_test, False)\n",
    "preprocessed_test_data = []\n",
    "def embed_test_sentences(sentence):\n",
    "    X_embs = [x[1][0] for x in sentence]\n",
    "    X_embs = np.asarray(X_embs)\n",
    "    return X_embs\n",
    "\n",
    "def preprocess_unlabelled_test_data(test_sentences):\n",
    "    for sentence in test_sentences:\n",
    "        sentence = transform_test_sentence(sentence, 1)\n",
    "        embedded = embed_test_sentences(sentence)\n",
    "        preprocessed_test_data.append(embedded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8d6ed0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_unlabelled_test_data(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1b78dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = []\n",
    "arg_max_dict = []\n",
    "def test_set_predictions(preprocessed_test_data, test_sentences):\n",
    "    for sentence in preprocessed_test_data:\n",
    "        predict_x=model.predict(sentence, batch_size=1, verbose=0) \n",
    "        predict_x = np.argmax(predict_x, axis=1)\n",
    "        arg_max_dict.append(predict_x)\n",
    "        \n",
    "    for index in range(len(test_sentences)):\n",
    "        predicted_sen = list(zip(test_sentences[index], le.inverse_transform(arg_max_dict[index])))\n",
    "        predicted_data.append(predicted_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5e3da54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions(preprocessed_test_data, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5a0ea441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_test_set(correct_set):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for predicted_sentence, correct_sentence in zip(predicted_data, correct_set):\n",
    "        for predicted_word, correct_word in zip(predicted_sentence, correct_sentence):\n",
    "            total = total + 1\n",
    "            if predicted_word[1] == correct_word[1]:\n",
    "                correct = correct + 1\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dd054b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.45771154779746"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_with_test_set(correct_test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a63e5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = []\n",
    "def generate_labelled_from_unlabelled(predicted_data, correct_sen):\n",
    "    for predicted_sentence, correct_sentence in zip(predicted_data, correct_sen):\n",
    "        for predicted_word, correct_word in zip(predicted_sentence, correct_sentence):\n",
    "            labelled_data.append((predicted_word[0], predicted_word[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "87856ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_labelled_from_unlabelled(predicted_data, correct_test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fb3edcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47377"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([item for sublist in test_sentences for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f43eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
