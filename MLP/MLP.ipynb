{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d58cbf4",
   "metadata": {},
   "source": [
    "# MLP for POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fed366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 15:31:32.935856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import pyconll, keras, pickle, os, random, nltk, datetime, warnings, gc, urllib.request, zipfile\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.sparse import hstack, vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, BatchNormalization, Dropout, Input, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# automatic module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(3) \n",
    "random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cacb339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path for imports to work\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# src imports\n",
    "from src.utils import get_root_dir, untag\n",
    "from src.parser import format_data, embeddings_init\n",
    "from src.data_helpers import vectorize, preprocess_unlabelled_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b57432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions \n",
    "def compare_with_test_set(predicted_data, correct_set):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for predicted_sentence, correct_sentence in zip(predicted_data, correct_set):\n",
    "        for predicted_word, correct_word in zip(predicted_sentence, correct_sentence):\n",
    "            total = total + 1\n",
    "            if predicted_word[1] == correct_word[1]:\n",
    "                #print(predicted_word[0], predicted_word[1], correct_word[1])\n",
    "                correct = correct + 1\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1297ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = get_root_dir() \n",
    "POS_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "pos_train = os.path.join(POS_DIR, \"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b01094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = format_data(pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517a6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences in train set:  8936\n",
      "Tagged words in train set: 211727\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08f7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset Split\n",
    "# total_train = len(train_sentences)\n",
    "# print(total_train)\n",
    "# val_split = math.floor(0.2 * total_train)\n",
    "# print(val_split)\n",
    "# train_sentences, val_sentences = train_sentences[:(total_train-val_split)], train_sentences[(total_train-val_split):]\n",
    "# print(len(train_sentences), len(val_sentences))\n",
    "# print(len(train_sentences)+len(val_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b72eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_sentences[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0103b9e4",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b702a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings\n",
      "Initialiation completed\n",
      "Embeddings loaded in 250.935922 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize Embeddings\n",
    "embeddings_path = os.path.join(get_root_dir(), 'dataset')\n",
    "t_ini = datetime.datetime.now()\n",
    "print('Initializing embeddings')\n",
    "embeddings = embeddings_init(str(embeddings_path))\n",
    "print('Initialiation completed')\n",
    "t_fin = datetime.datetime.now()\n",
    "print('Embeddings loaded in {} seconds'.format((t_fin - t_ini).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa731631",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c = dict()\n",
    "for item in embeddings.key_to_index:\n",
    "    w2c[item] = embeddings.key_to_index[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e31b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)\n",
    "np.random.seed(3)\n",
    "oov = np.random.uniform(-0.25, 0.25, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fc6484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings window method\n",
      "Vectorizing Dataset...\n",
      "Vectorizing train...\n",
      "Dataset vectorized.\n",
      "Train shape: (211727, 900)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = vectorize(embeddings, oov, train_sentences, window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b21e1b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211727, 900)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1929a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(list(set(y_train)))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce5df202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3039440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211727, 44)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "# y_val = le.transform(y_val)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "# y_val = keras.utils.to_categorical(y_val)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b442450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # In the first layer, we specify the input data shape\n",
    "\n",
    "# model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "419d05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_epoch = 10\n",
    "# batch_size = 128\n",
    "# early_stopping = EarlyStopping(monitor = 'val_acc', patience = 5)\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     epochs=nb_epoch,\n",
    "#                     batch_size=batch_size,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     verbose=1,\n",
    "#                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f65847b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_full_train = vstack((X_train, X_val)).tocsr()\n",
    "# y_full_train = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa039ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169381, 70531)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "100c9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_model(input_dim):\n",
    "    \"\"\"\n",
    "    Define a MLP model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(512, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786d7fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               461312    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 44)                2860      \n",
      "=================================================================\n",
      "Total params: 497,004\n",
      "Trainable params: 497,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_mlp_model(X_train.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ef22568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 14:46:07.610300: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 762217200 exceeds 10% of free system memory.\n",
      "2023-03-11 14:46:08.757465: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-03-11 14:46:08.791268: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1700000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 14:46:10.153685: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6617/6617 [==============================] - 35s 4ms/step - loss: 0.7402 - accuracy: 0.7988\n",
      "Epoch 2/20\n",
      "6617/6617 [==============================] - 26s 4ms/step - loss: 0.2310 - accuracy: 0.9305\n",
      "Epoch 3/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1890 - accuracy: 0.9426\n",
      "Epoch 4/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1695 - accuracy: 0.9473\n",
      "Epoch 5/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1593 - accuracy: 0.9503\n",
      "Epoch 6/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1480 - accuracy: 0.9530\n",
      "Epoch 7/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1421 - accuracy: 0.9550\n",
      "Epoch 8/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1368 - accuracy: 0.9576\n",
      "Epoch 9/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1290 - accuracy: 0.9602\n",
      "Epoch 10/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1248 - accuracy: 0.9604\n",
      "Epoch 11/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1209 - accuracy: 0.9621\n",
      "Epoch 12/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1192 - accuracy: 0.9627\n",
      "Epoch 13/20\n",
      "6617/6617 [==============================] - 26s 4ms/step - loss: 0.1172 - accuracy: 0.9637\n",
      "Epoch 14/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1161 - accuracy: 0.9648\n",
      "Epoch 15/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1106 - accuracy: 0.9652\n",
      "Epoch 16/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1120 - accuracy: 0.9649\n",
      "Epoch 17/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1068 - accuracy: 0.9669\n",
      "Epoch 18/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1069 - accuracy: 0.9669\n",
      "Epoch 19/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1038 - accuracy: 0.9676\n",
      "Epoch 20/20\n",
      "6617/6617 [==============================] - 27s 4ms/step - loss: 0.1044 - accuracy: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96f81d4790>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=1,\n",
    "                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "985551b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31e84590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 21:59:31.257721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-10 21:59:31.257913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-10 21:59:31.260761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-10 21:59:31.261050: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-10 21:59:31.261091: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-10 21:59:31.261452: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-10 21:59:31.261777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-10 21:59:31.262053: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-10 21:59:31.262333: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-10 21:59:31.262343: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-10 21:59:31.266897: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# model = keras.models.load_model(save_format='h5', \"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6d2ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = os.path.join(POS_DIR, \"test.txt\")\n",
    "\n",
    "test_sentences = format_data(pos_test, False)\n",
    "\n",
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b949bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_correct = os.path.join(POS_DIR, \"test_labelled.txt\") \n",
    "correct_test_sen = format_data(pos_test_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fd0f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(predicted_sen, correct_sen, acc = True):\n",
    "    total = 0\n",
    "    correct_total = 0\n",
    "    if acc:\n",
    "        for predict, correct in zip(predicted_sen, correct_sen):\n",
    "                if predict[1] == correct[1]:\n",
    "                    correct_total = correct_total + 1\n",
    "                total = total + 1\n",
    "        acc = correct_total / total\n",
    "        return acc\n",
    "    else:\n",
    "        for predict, correct in zip(predicted_sen, correct_sen):\n",
    "                if predict[1] == correct[1]:\n",
    "                    print(f\"{predict[1]} \\t {correct[1]}\")\n",
    "                else:\n",
    "                    print(f\"{predict[1]} \\t {correct[1]} <------ Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "154290dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = format_data(pos_test, False)\n",
    "\n",
    "preprocessed_test_data = preprocess_unlabelled_test_data(embeddings, oov, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b78dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = []\n",
    "arg_max_dict = []\n",
    "def test_set_predictions(preprocessed_test_data, test_sentences):\n",
    "    for sentence in preprocessed_test_data:\n",
    "        predict_x=model.predict(sentence, batch_size=1, verbose=0) \n",
    "        predict_x = np.argmax(predict_x, axis=1)\n",
    "        arg_max_dict.append(predict_x)\n",
    "        \n",
    "    for index in range(len(test_sentences)):\n",
    "        predicted_sen = list(zip(test_sentences[index], le.inverse_transform(arg_max_dict[index])))\n",
    "        predicted_data.append(predicted_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e3da54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions(preprocessed_test_data, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69d639ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('rockwell', 'NNP'),\n",
       "  ('international', 'NNP'),\n",
       "  ('corp.', 'NNP'),\n",
       "  (\"'s\", 'DT'),\n",
       "  ('tulsa', 'NNP'),\n",
       "  ('unit', 'VBP'),\n",
       "  ('said', 'VBD'),\n",
       "  ('it', 'PRP'),\n",
       "  ('signed', 'VBD'),\n",
       "  ('a', 'DT'),\n",
       "  ('tentative', 'JJ'),\n",
       "  ('agreement', 'NN'),\n",
       "  ('extending', 'VBG'),\n",
       "  ('its', 'PRP$'),\n",
       "  ('contract', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('boeing', 'NNP'),\n",
       "  ('co.', 'NNP'),\n",
       "  ('to', 'TO'),\n",
       "  ('provide', 'VB'),\n",
       "  ('structural', 'JJ'),\n",
       "  ('parts', 'NNS'),\n",
       "  ('for', 'IN'),\n",
       "  ('boeing', 'NNP'),\n",
       "  (\"'s\", 'POS'),\n",
       "  ('747', 'NN'),\n",
       "  ('jetliners', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('rockwell', 'NNP'),\n",
       "  ('said', 'VBD'),\n",
       "  ('the', 'DT'),\n",
       "  ('agreement', 'NN'),\n",
       "  ('calls', 'VBZ'),\n",
       "  ('for', 'IN'),\n",
       "  ('it', 'PRP'),\n",
       "  ('to', 'TO'),\n",
       "  ('supply', 'VB'),\n",
       "  ('200', 'CD'),\n",
       "  ('additional', 'JJ'),\n",
       "  ('so-called', 'JJ'),\n",
       "  ('shipsets', 'NNP'),\n",
       "  ('for', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('planes', 'NNS'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd054b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.89463241657344"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_with_test_set(predicted_data, correct_test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d848310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labelled_data(file_name):\n",
    "    f = open(file_name, \"w\")\n",
    "    for sentence in predicted_data:\n",
    "        for word, pos in sentence:\n",
    "            f.write(f\"{word} {pos}\\n\")\n",
    "        f.write(f\"\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "26b82d29683d0e2e5d2cbb75bad0064bccae605df6fb9f9bb51c3186d8a354e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
