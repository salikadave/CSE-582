{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fed366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:18:22.833129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 22:18:23.069469: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:18:23.069679: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-12 22:18:24.662523: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:18:24.662574: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:18:24.662580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pyconll, keras, pickle, os, random, nltk, datetime, warnings, gc, urllib.request, zipfile\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.sparse import hstack, vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, BatchNormalization, Dropout, Input, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1297ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the directories and files - this is local\n",
    "# ROOT_DIR = os.path.dirname(\"/home/chitrang/Documents/CSE-582/\")\n",
    "# POS_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "# pos_train = os.path.join(POS_DIR, \"train.txt\")\n",
    "\n",
    "# Load the directories and files - this is relative\n",
    "pos_train = './../dataset/train.txt'\n",
    "pos_test = './../dataset/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0e37e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74586842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(fname, include_y=True):\n",
    "    sentences = [] # master list\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    sentence = [] # local list\n",
    "    for line in content:\n",
    "        if line !='\\n':\n",
    "            line = line.strip() # remove leading/trailing spaces\n",
    "            word = line.split()[0].lower() # get the word\n",
    "            if include_y:\n",
    "                pos = \"\"\n",
    "                pos = line.split()[1] # get the pos tag\n",
    "                sentence.append((word, pos)) # create a pair and save to local list\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "        else:\n",
    "            sentences.append(sentence) # once a \\n is detected, append the local sentence to master sentence\n",
    "            sentence = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b01094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = format_data(pos_train)\n",
    "test_sentences = format_data(pos_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517a6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences in train set:  8936\n",
      "Tagged words in train set: 211727\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3bf2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sequence(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def text_sequence(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def id2word(sentences):\n",
    "    wordlist = [item for sublist in text_sequence(sentences) for item in sublist]\n",
    "    id2word = {k:v for k,v in enumerate(wordlist)}\n",
    "    return id2word\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def untag_pos(tagged_sentence):\n",
    "    return [t for _, t in tagged_sentence]\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab =set()\n",
    "    for sentence in sentences:\n",
    "        for word in untag(sentence):\n",
    "            vocab.add(word)\n",
    "    return sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b09d5",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b702a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_path = '/home/chitrang/Downloads/wiki-news-300d-1M.vec'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embs_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa731631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary - just for fun!\n",
    "w2c = dict()\n",
    "for item in embeddings.key_to_index:\n",
    "    w2c[item] = embeddings.key_to_index[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e31b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)\n",
    "np.random.seed(3)\n",
    "oov = np.random.uniform(-0.25, 0.25, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf29db",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6722609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_embs(sentence, index, window=2):\n",
    "    unknown=0\n",
    "    vec = np.array([])\n",
    "    for i in range(index-window, index+window+1):\n",
    "#         if i < 0:\n",
    "#             vec = np.append(vec, pad)\n",
    "#         if i > len(sentence)-1:\n",
    "#             vec = np.append(vec, pad)\n",
    "        try:\n",
    "            vec = np.append(vec, embeddings[sentence[i]])\n",
    "        except:\n",
    "            vec = np.append(vec, oov)\n",
    "            unknown += 1\n",
    "    return vec, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a597ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_basic(sentence, index):\n",
    "    return {\n",
    "        'nb_terms': len(sentence),        \n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'i-1_prefix-3': '' if index == 0 else sentence[index-1][:3],        \n",
    "        'i-1_suffix-3': '' if index == 0 else sentence[index-1][-3:],        \n",
    "        'i+1_prefix-3': '' if index == len(sentence) - 1 else sentence[index+1][:3],        \n",
    "        'i+1_suffix-3': '' if index == len(sentence) - 1 else sentence[index+1][-3:],        \n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59266ca9",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21bfa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences, window):\n",
    "    i=0\n",
    "    X, y = [], []\n",
    "    for doc_index, tagged in enumerate(tagged_sentences):\n",
    "        for index in range(len(tagged)):\n",
    "            X.append([features_basic(untag(tagged), index),\\\n",
    "                      features_embs(untag(tagged), index, window)[0],\\\n",
    "                     ])\n",
    "            y.append(tagged[index][1])\n",
    "            k = features_embs(untag(tagged), index, window)[1]\n",
    "            i += k\n",
    "    return X, y, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e526dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_sentence(sentence, window):\n",
    "    X = []\n",
    "    for index in range(len(sentence)):\n",
    "            X.append([\n",
    "                      features_basic(sentence, index),\\\n",
    "                      features_embs(sentence, index, window),\\\n",
    "                     ])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38bc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train, window=2):\n",
    "    X_train, y_train, unk_tr = transform_to_dataset(train, window=window)\n",
    "    X_train = [x[1] for x in X_train]\n",
    "    X_train = np.asarray(X_train)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc6484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = vectorize(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21e1b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211727, 1500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1929a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(list(set(y_train)))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce5df202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset split: Train - Validation\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b37b84",
   "metadata": {},
   "source": [
    "### One hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3039440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211727, 44)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "# y_val = le.transform(y_val)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "# y_val = keras.utils.to_categorical(y_val)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0475c",
   "metadata": {},
   "source": [
    "### Train + Val model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b442450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # In the first layer, we specify the input data shape\n",
    "\n",
    "# model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "419d05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_epoch = 10\n",
    "# batch_size = 128\n",
    "# early_stopping = EarlyStopping(monitor = 'val_acc', patience = 5)\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     epochs=nb_epoch,\n",
    "#                     batch_size=batch_size,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     verbose=1,\n",
    "#                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f65847b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_full_train = vstack((X_train, X_val)).tocsr()\n",
    "# y_full_train = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa039ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6737d",
   "metadata": {},
   "source": [
    "### Full Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "100c9795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 18:37:28.652704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-12 18:37:28.652909: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.656450: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.656703: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.656750: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.656817: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.657130: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.657374: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.657606: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.657615: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-12 18:37:28.665314: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               768512    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 44)                2860      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 804,204\n",
      "Trainable params: 804,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ef22568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 18:37:33.298584: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1270362000 exceeds 10% of free system memory.\n",
      "2023-03-12 18:37:35.607576: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 37263952 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6617/6617 [==============================] - 44s 7ms/step - loss: 0.4496 - accuracy: 0.8721\n",
      "Epoch 2/20\n",
      "6617/6617 [==============================] - 35s 5ms/step - loss: 0.2327 - accuracy: 0.9296\n",
      "Epoch 3/20\n",
      "6617/6617 [==============================] - 52s 8ms/step - loss: 0.1982 - accuracy: 0.9392\n",
      "Epoch 4/20\n",
      "6617/6617 [==============================] - 42s 6ms/step - loss: 0.1774 - accuracy: 0.9453\n",
      "Epoch 5/20\n",
      "6617/6617 [==============================] - 39s 6ms/step - loss: 0.1605 - accuracy: 0.9501\n",
      "Epoch 6/20\n",
      "6617/6617 [==============================] - 45s 7ms/step - loss: 0.1499 - accuracy: 0.9538\n",
      "Epoch 7/20\n",
      "6617/6617 [==============================] - 44s 7ms/step - loss: 0.1432 - accuracy: 0.9557\n",
      "Epoch 8/20\n",
      "6617/6617 [==============================] - 49s 7ms/step - loss: 0.1347 - accuracy: 0.9585\n",
      "Epoch 9/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1264 - accuracy: 0.9608\n",
      "Epoch 10/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1222 - accuracy: 0.9626\n",
      "Epoch 11/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1160 - accuracy: 0.9640\n",
      "Epoch 12/20\n",
      "6617/6617 [==============================] - 50s 8ms/step - loss: 0.1139 - accuracy: 0.9652\n",
      "Epoch 13/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1085 - accuracy: 0.9669\n",
      "Epoch 14/20\n",
      "6617/6617 [==============================] - 50s 8ms/step - loss: 0.1093 - accuracy: 0.9672\n",
      "Epoch 15/20\n",
      "6617/6617 [==============================] - 40s 6ms/step - loss: 0.1043 - accuracy: 0.9684\n",
      "Epoch 16/20\n",
      "6617/6617 [==============================] - 44s 7ms/step - loss: 0.1012 - accuracy: 0.9691\n",
      "Epoch 17/20\n",
      "6617/6617 [==============================] - 45s 7ms/step - loss: 0.1002 - accuracy: 0.9699\n",
      "Epoch 18/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1004 - accuracy: 0.9704\n",
      "Epoch 19/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.0948 - accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "6617/6617 [==============================] - 40s 6ms/step - loss: 0.0942 - accuracy: 0.9723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e99d19ca0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=1,\n",
    "                   ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da2a7af",
   "metadata": {},
   "source": [
    "### Save and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "985551b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"mlp_model.h5\", save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e84590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:21:45.840530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-12 22:21:45.840723: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:21:45.840777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:21:45.840814: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:21:45.840847: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:21:45.840880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:21:45.840912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:21:45.840946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:21:45.840979: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-12 22:21:45.840985: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-12 22:21:45.841313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"mlp_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "802fbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim = embeddings.vectors.shape[1]\n",
    "# pad = np.zeros(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "154290dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_data = []\n",
    "# Embed test data\n",
    "def embed_test_sentences(sentence):\n",
    "    X_embs = [x[1][0] for x in sentence]\n",
    "    X_embs = np.asarray(X_embs)\n",
    "    return X_embs\n",
    "\n",
    "# Preprocess Unlabelled Data\n",
    "def preprocess_unlabelled_test_data(test_sentences):\n",
    "    for sentence in test_sentences:\n",
    "        sentence = transform_test_sentence(sentence, 2)\n",
    "        embedded = embed_test_sentences(sentence)\n",
    "        preprocessed_test_data.append(embedded)\n",
    "        \n",
    "def generate_labelled_data(file_name):\n",
    "    f = open(file_name, \"w\")\n",
    "    for sentence in predicted_data:\n",
    "        for word, pos in sentence:\n",
    "            f.write(f\"{word} {pos}\\n\")\n",
    "        f.write(f\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6ed0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_unlabelled_test_data(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b78dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = []\n",
    "arg_max_dict = []\n",
    "def test_set_predictions(preprocessed_test_data, test_sentences):\n",
    "    for sentence in preprocessed_test_data:\n",
    "        predict_x=model.predict(sentence, batch_size=1, verbose=0) \n",
    "        predict_x = np.argmax(predict_x, axis=1)\n",
    "        arg_max_dict.append(predict_x)\n",
    "        \n",
    "    for index in range(len(test_sentences)):\n",
    "        predicted_sen = list(zip(test_sentences[index], le.inverse_transform(arg_max_dict[index])))\n",
    "        predicted_data.append(predicted_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3da54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions(preprocessed_test_data, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e50c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_labelled_data('./../Labelled_outputs/mlp_labelled_best.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
