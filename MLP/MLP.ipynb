{
 "cells": [
  {
<<<<<<< Updated upstream
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d58cbf4",
   "metadata": {},
   "source": [
    "# MLP for POS Tagging"
   ]
  },
  {
=======
>>>>>>> Stashed changes
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fed366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "2023-03-11 17:39:10.596207: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
=======
      "2023-03-12 17:46:28.283859: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 17:46:28.592923: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-12 17:46:28.592942: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-12 17:46:30.454333: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 17:46:30.454388: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 17:46:30.454394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "import pyconll, keras, pickle, os, random, nltk, datetime, warnings, gc, urllib.request, zipfile\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.sparse import hstack, vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, BatchNormalization, Dropout, Input, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from numpy.random import seed\n",
<<<<<<< Updated upstream
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# automatic module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(3) \n",
    "random.seed(3)"
=======
    "from sklearn.model_selection import train_test_split"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 44,
   "id": "cacb339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path for imports to work\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# src imports\n",
    "from src.utils import get_root_dir, untag\n",
    "from src.parser import format_data, embeddings_init\n",
    "from src.data_helpers import vectorize, preprocess_unlabelled_test_data"
=======
   "execution_count": 2,
   "id": "1297ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the directories and files - this is local\n",
    "# ROOT_DIR = os.path.dirname(\"/home/chitrang/Documents/CSE-582/\")\n",
    "# POS_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "# pos_train = os.path.join(POS_DIR, \"train.txt\")\n",
    "\n",
    "# Load the directories and files - this is relative\n",
    "pos_train = './../dataset/train.txt'\n",
    "pos_test = './../dataset/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46659250",
   "metadata": {},
   "source": [
    "## Data Preparation"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< Updated upstream
   "id": "2b57432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions \n",
    "def compare_with_test_set(predicted_data, correct_set):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for predicted_sentence, correct_sentence in zip(predicted_data, correct_set):\n",
    "        for predicted_word, correct_word in zip(predicted_sentence, correct_sentence):\n",
    "            total = total + 1\n",
    "            if predicted_word[1] == correct_word[1]:\n",
    "                #print(predicted_word[0], predicted_word[1], correct_word[1])\n",
    "                correct = correct + 1\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy"
=======
   "id": "74586842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(fname, include_y=True):\n",
    "    sentences = [] # master list\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    sentence = [] # local list\n",
    "    for line in content:\n",
    "        if line !='\\n':\n",
    "            line = line.strip() # remove leading/trailing spaces\n",
    "            word = line.split()[0].lower() # get the word\n",
    "            if include_y:\n",
    "                pos = \"\"\n",
    "                pos = line.split()[1] # get the pos tag\n",
    "                sentence.append((word, pos)) # create a pair and save to local list\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "        else:\n",
    "            sentences.append(sentence) # once a \\n is detected, append the local sentence to master sentence\n",
    "            sentence = []\n",
    "    return sentences"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
<<<<<<< Updated upstream
   "id": "1297ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = get_root_dir() \n",
    "POS_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "pos_train = os.path.join(POS_DIR, \"train.txt\")"
=======
   "id": "85b01094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = format_data(pos_train)\n",
    "test_sentences = format_data(pos_test, False)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
<<<<<<< Updated upstream
   "id": "85b01094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = format_data(pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
=======
>>>>>>> Stashed changes
   "id": "517a6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences in train set:  8936\n",
      "Tagged words in train set: 211727\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "b08f7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset Split\n",
    "# total_train = len(train_sentences)\n",
    "# print(total_train)\n",
    "# val_split = math.floor(0.2 * total_train)\n",
    "# print(val_split)\n",
    "# train_sentences, val_sentences = train_sentences[:(total_train-val_split)], train_sentences[(total_train-val_split):]\n",
    "# print(len(train_sentences), len(val_sentences))\n",
    "# print(len(train_sentences)+len(val_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b72eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_sentences[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0103b9e4",
   "metadata": {},
   "source": [
    "### Embeddings"
=======
   "execution_count": 6,
   "id": "da3bf2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sequence(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def text_sequence(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def id2word(sentences):\n",
    "    wordlist = [item for sublist in text_sequence(sentences) for item in sublist]\n",
    "    id2word = {k:v for k,v in enumerate(wordlist)}\n",
    "    return id2word\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def untag_pos(tagged_sentence):\n",
    "    return [t for _, t in tagged_sentence]\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab =set()\n",
    "    for sentence in sentences:\n",
    "        for word in untag(sentence):\n",
    "            vocab.add(word)\n",
    "    return sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af083958",
   "metadata": {},
   "source": [
    "### Load embeddings"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b702a6",
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings\n",
      "Initialiation completed\n",
      "Embeddings loaded in 249.977289 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize Embeddings\n",
    "embeddings_path = os.path.join(get_root_dir(), 'dataset')\n",
    "t_ini = datetime.datetime.now()\n",
    "print('Initializing embeddings')\n",
    "embeddings = embeddings_init(str(embeddings_path))\n",
    "print('Initialiation completed')\n",
    "t_fin = datetime.datetime.now()\n",
    "print('Embeddings loaded in {} seconds'.format((t_fin - t_ini).total_seconds()))"
=======
   "outputs": [],
   "source": [
    "embs_path = '/home/chitrang/Downloads/wiki-news-300d-1M.vec'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embs_path, binary=False)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa731631",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
=======
    "# Create a dictionary - just for fun!\n",
>>>>>>> Stashed changes
    "w2c = dict()\n",
    "for item in embeddings.key_to_index:\n",
    "    w2c[item] = embeddings.key_to_index[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e31b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)\n",
    "np.random.seed(3)\n",
    "oov = np.random.uniform(-0.25, 0.25, dim)"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": 80,
   "id": "97722908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "WINDOW_SIZE = 2"
=======
   "cell_type": "markdown",
   "id": "c99c7cee",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6722609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_embs(sentence, index, window=2):\n",
    "    unknown=0\n",
    "    vec = np.array([])\n",
    "    for i in range(index-window, index+window+1):\n",
    "#         if i < 0:\n",
    "#             vec = np.append(vec, pad)\n",
    "#         if i > len(sentence)-1:\n",
    "#             vec = np.append(vec, pad)\n",
    "        try:\n",
    "            vec = np.append(vec, embeddings[sentence[i]])\n",
    "        except:\n",
    "            vec = np.append(vec, oov)\n",
    "            unknown += 1\n",
    "    return vec, unknown"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 78,
   "id": "5fc6484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings window method\n",
      "Vectorizing Dataset...\n",
      "Vectorizing train...\n",
      "Dataset vectorized.\n",
      "Train shape: (211727, 1500)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = vectorize(embeddings, oov, train_sentences, window=WINDOW_SIZE)"
=======
   "execution_count": 11,
   "id": "5a597ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_basic(sentence, index):\n",
    "    return {\n",
    "        'nb_terms': len(sentence),        \n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'i-1_prefix-3': '' if index == 0 else sentence[index-1][:3],        \n",
    "        'i-1_suffix-3': '' if index == 0 else sentence[index-1][-3:],        \n",
    "        'i+1_prefix-3': '' if index == len(sentence) - 1 else sentence[index+1][:3],        \n",
    "        'i+1_suffix-3': '' if index == len(sentence) - 1 else sentence[index+1][-3:],        \n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687bd3d",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21bfa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences, window):\n",
    "    i=0\n",
    "    X, y = [], []\n",
    "    for doc_index, tagged in enumerate(tagged_sentences):\n",
    "        for index in range(len(tagged)):\n",
    "            X.append([features_basic(untag(tagged), index),\\\n",
    "                      features_embs(untag(tagged), index, window)[0],\\\n",
    "                     ])\n",
    "            y.append(tagged[index][1])\n",
    "            k = features_embs(untag(tagged), index, window)[1]\n",
    "            i += k\n",
    "    return X, y, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e526dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_sentence(sentence, window):\n",
    "    X = []\n",
    "    for index in range(len(sentence)):\n",
    "            X.append([\n",
    "                      features_basic(sentence, index),\\\n",
    "                      features_embs(sentence, index, window),\\\n",
    "                     ])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38bc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train, window=2):\n",
    "    X_train, y_train, unk_tr = transform_to_dataset(train, window=window)\n",
    "    X_train = [x[1] for x in X_train]\n",
    "    X_train = np.asarray(X_train)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc6484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = vectorize(train_sentences)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 79,
=======
   "execution_count": 18,
>>>>>>> Stashed changes
   "id": "b21e1b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211727, 1500)"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 79,
=======
     "execution_count": 18,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 81,
=======
   "execution_count": 16,
>>>>>>> Stashed changes
   "id": "1929a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(list(set(y_train)))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 17,
>>>>>>> Stashed changes
   "id": "ce5df202",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
=======
    "# Dataset split: Train - Validation\n",
>>>>>>> Stashed changes
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": 82,
=======
   "cell_type": "markdown",
   "id": "44b9d8b9",
   "metadata": {},
   "source": [
    "### One hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
>>>>>>> Stashed changes
   "id": "f3039440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211727, 44)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "# y_val = le.transform(y_val)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "# y_val = keras.utils.to_categorical(y_val)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": null,
=======
   "cell_type": "markdown",
   "id": "3f1fef74",
   "metadata": {},
   "source": [
    "### Train + Val model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
>>>>>>> Stashed changes
   "id": "4b442450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # In the first layer, we specify the input data shape\n",
    "\n",
    "# model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 20,
>>>>>>> Stashed changes
   "id": "419d05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_epoch = 10\n",
    "# batch_size = 128\n",
    "# early_stopping = EarlyStopping(monitor = 'val_acc', patience = 5)\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     epochs=nb_epoch,\n",
    "#                     batch_size=batch_size,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     verbose=1,\n",
    "#                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 21,
>>>>>>> Stashed changes
   "id": "f65847b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_full_train = vstack((X_train, X_val)).tocsr()\n",
    "# y_full_train = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 22,
>>>>>>> Stashed changes
   "id": "aa039ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": 83,
   "id": "100c9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_model(input_dim):\n",
    "    \"\"\"\n",
    "    Define a MLP model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(512, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
=======
   "cell_type": "markdown",
   "id": "7dffa65e",
   "metadata": {},
   "source": [
    "### Full Training Set"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 84,
   "id": "786d7fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 512)               768512    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 44)                5676      \n",
      "=================================================================\n",
      "Total params: 815,340\n",
      "Trainable params: 815,340\n",
=======
   "execution_count": 23,
   "id": "100c9795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 18:37:28.652704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-12 18:37:28.652909: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.656450: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.656703: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.656750: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.656817: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.657130: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.657374: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.657606: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-12 18:37:28.657615: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-12 18:37:28.665314: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               768512    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 44)                2860      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 804,204\n",
      "Trainable params: 804,204\n",
>>>>>>> Stashed changes
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "model = get_mlp_model(X_train.shape[1])\n",
=======
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
>>>>>>> Stashed changes
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 85,
=======
   "execution_count": 24,
>>>>>>> Stashed changes
   "id": "5ef22568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "2023-03-11 19:20:45.077315: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1270362000 exceeds 10% of free system memory.\n"
=======
      "2023-03-12 18:37:33.298584: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1270362000 exceeds 10% of free system memory.\n",
      "2023-03-12 18:37:35.607576: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 37263952 exceeds 10% of free system memory.\n"
>>>>>>> Stashed changes
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Epoch 1/100\n",
      "828/828 [==============================] - 5s 5ms/step - loss: 1.1264 - accuracy: 0.6968\n",
      "Epoch 2/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.2323 - accuracy: 0.9313\n",
      "Epoch 3/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.1649 - accuracy: 0.9509\n",
      "Epoch 4/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.1264 - accuracy: 0.9608\n",
      "Epoch 5/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.1060 - accuracy: 0.9674\n",
      "Epoch 6/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0861 - accuracy: 0.9728\n",
      "Epoch 7/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0787 - accuracy: 0.9753\n",
      "Epoch 8/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0678 - accuracy: 0.9786\n",
      "Epoch 9/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0573 - accuracy: 0.9819\n",
      "Epoch 10/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0550 - accuracy: 0.9821\n",
      "Epoch 11/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0505 - accuracy: 0.9841\n",
      "Epoch 12/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0445 - accuracy: 0.9859\n",
      "Epoch 13/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0413 - accuracy: 0.9867\n",
      "Epoch 14/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0406 - accuracy: 0.9868\n",
      "Epoch 15/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0367 - accuracy: 0.9882\n",
      "Epoch 16/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0370 - accuracy: 0.9885\n",
      "Epoch 17/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0324 - accuracy: 0.9897\n",
      "Epoch 18/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0321 - accuracy: 0.9899\n",
      "Epoch 19/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0302 - accuracy: 0.9907\n",
      "Epoch 20/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0289 - accuracy: 0.9909\n",
      "Epoch 21/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0270 - accuracy: 0.9915\n",
      "Epoch 22/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0267 - accuracy: 0.9913\n",
      "Epoch 23/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0251 - accuracy: 0.9916\n",
      "Epoch 24/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0253 - accuracy: 0.9919\n",
      "Epoch 25/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0244 - accuracy: 0.9923\n",
      "Epoch 26/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0224 - accuracy: 0.9929\n",
      "Epoch 27/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0225 - accuracy: 0.9930\n",
      "Epoch 28/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0214 - accuracy: 0.9934\n",
      "Epoch 29/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0239 - accuracy: 0.9923\n",
      "Epoch 30/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0213 - accuracy: 0.9933\n",
      "Epoch 31/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0203 - accuracy: 0.9935\n",
      "Epoch 32/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0190 - accuracy: 0.9939\n",
      "Epoch 33/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0213 - accuracy: 0.9933\n",
      "Epoch 34/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0199 - accuracy: 0.9937\n",
      "Epoch 35/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0195 - accuracy: 0.9941\n",
      "Epoch 36/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0183 - accuracy: 0.9940\n",
      "Epoch 37/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0169 - accuracy: 0.9948\n",
      "Epoch 38/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0196 - accuracy: 0.9940\n",
      "Epoch 39/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0176 - accuracy: 0.9946\n",
      "Epoch 40/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0171 - accuracy: 0.9947\n",
      "Epoch 41/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0165 - accuracy: 0.9950\n",
      "Epoch 42/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0169 - accuracy: 0.9948\n",
      "Epoch 43/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0156 - accuracy: 0.9953\n",
      "Epoch 44/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0158 - accuracy: 0.9950\n",
      "Epoch 45/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0154 - accuracy: 0.9951\n",
      "Epoch 46/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0155 - accuracy: 0.9954\n",
      "Epoch 47/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0145 - accuracy: 0.9954\n",
      "Epoch 48/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0156 - accuracy: 0.9952\n",
      "Epoch 49/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0162 - accuracy: 0.9949\n",
      "Epoch 50/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0145 - accuracy: 0.9956\n",
      "Epoch 51/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0155 - accuracy: 0.9952\n",
      "Epoch 52/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0146 - accuracy: 0.9956\n",
      "Epoch 53/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0143 - accuracy: 0.9955\n",
      "Epoch 54/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0151 - accuracy: 0.9953\n",
      "Epoch 55/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0160 - accuracy: 0.9951\n",
      "Epoch 56/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0139 - accuracy: 0.9960\n",
      "Epoch 57/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0136 - accuracy: 0.9958\n",
      "Epoch 58/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0127 - accuracy: 0.9962\n",
      "Epoch 59/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0121 - accuracy: 0.9961\n",
      "Epoch 60/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0136 - accuracy: 0.9960\n",
      "Epoch 61/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0135 - accuracy: 0.9963\n",
      "Epoch 62/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0137 - accuracy: 0.9960\n",
      "Epoch 63/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0138 - accuracy: 0.9960\n",
      "Epoch 64/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0129 - accuracy: 0.9962\n",
      "Epoch 65/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0135 - accuracy: 0.9959\n",
      "Epoch 66/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0125 - accuracy: 0.9961\n",
      "Epoch 67/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0118 - accuracy: 0.9964\n",
      "Epoch 68/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 69/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0129 - accuracy: 0.9962\n",
      "Epoch 70/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0114 - accuracy: 0.9966\n",
      "Epoch 71/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0123 - accuracy: 0.9965\n",
      "Epoch 72/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0120 - accuracy: 0.9962\n",
      "Epoch 73/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0120 - accuracy: 0.9963\n",
      "Epoch 74/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0114 - accuracy: 0.9964\n",
      "Epoch 75/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0115 - accuracy: 0.9966\n",
      "Epoch 76/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0117 - accuracy: 0.9966\n",
      "Epoch 77/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0118 - accuracy: 0.9966\n",
      "Epoch 78/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0112 - accuracy: 0.9967\n",
      "Epoch 79/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 80/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0111 - accuracy: 0.9968\n",
      "Epoch 81/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0110 - accuracy: 0.9967\n",
      "Epoch 82/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0113 - accuracy: 0.9966\n",
      "Epoch 83/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0116 - accuracy: 0.9966\n",
      "Epoch 84/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0110 - accuracy: 0.9968\n",
      "Epoch 85/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0111 - accuracy: 0.9967\n",
      "Epoch 86/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0111 - accuracy: 0.9968\n",
      "Epoch 87/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0106 - accuracy: 0.9967\n",
      "Epoch 88/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0114 - accuracy: 0.9966\n",
      "Epoch 89/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0122 - accuracy: 0.9967\n",
      "Epoch 90/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0101 - accuracy: 0.9970\n",
      "Epoch 91/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0119 - accuracy: 0.9965\n",
      "Epoch 92/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0116 - accuracy: 0.9968\n",
      "Epoch 93/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0096 - accuracy: 0.9973\n",
      "Epoch 94/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0100 - accuracy: 0.9968\n",
      "Epoch 95/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0115 - accuracy: 0.9967\n",
      "Epoch 96/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0090 - accuracy: 0.9974\n",
      "Epoch 97/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0113 - accuracy: 0.9968\n",
      "Epoch 98/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0091 - accuracy: 0.9973\n",
      "Epoch 99/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0095 - accuracy: 0.9972\n",
      "Epoch 100/100\n",
      "828/828 [==============================] - 4s 5ms/step - loss: 0.0101 - accuracy: 0.9970\n"
=======
      "Epoch 1/20\n",
      "6617/6617 [==============================] - 44s 7ms/step - loss: 0.4496 - accuracy: 0.8721\n",
      "Epoch 2/20\n",
      "6617/6617 [==============================] - 35s 5ms/step - loss: 0.2327 - accuracy: 0.9296\n",
      "Epoch 3/20\n",
      "6617/6617 [==============================] - 52s 8ms/step - loss: 0.1982 - accuracy: 0.9392\n",
      "Epoch 4/20\n",
      "6617/6617 [==============================] - 42s 6ms/step - loss: 0.1774 - accuracy: 0.9453\n",
      "Epoch 5/20\n",
      "6617/6617 [==============================] - 39s 6ms/step - loss: 0.1605 - accuracy: 0.9501\n",
      "Epoch 6/20\n",
      "6617/6617 [==============================] - 45s 7ms/step - loss: 0.1499 - accuracy: 0.9538\n",
      "Epoch 7/20\n",
      "6617/6617 [==============================] - 44s 7ms/step - loss: 0.1432 - accuracy: 0.9557\n",
      "Epoch 8/20\n",
      "6617/6617 [==============================] - 49s 7ms/step - loss: 0.1347 - accuracy: 0.9585\n",
      "Epoch 9/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1264 - accuracy: 0.9608\n",
      "Epoch 10/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1222 - accuracy: 0.9626\n",
      "Epoch 11/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1160 - accuracy: 0.9640\n",
      "Epoch 12/20\n",
      "6617/6617 [==============================] - 50s 8ms/step - loss: 0.1139 - accuracy: 0.9652\n",
      "Epoch 13/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1085 - accuracy: 0.9669\n",
      "Epoch 14/20\n",
      "6617/6617 [==============================] - 50s 8ms/step - loss: 0.1093 - accuracy: 0.9672\n",
      "Epoch 15/20\n",
      "6617/6617 [==============================] - 40s 6ms/step - loss: 0.1043 - accuracy: 0.9684\n",
      "Epoch 16/20\n",
      "6617/6617 [==============================] - 44s 7ms/step - loss: 0.1012 - accuracy: 0.9691\n",
      "Epoch 17/20\n",
      "6617/6617 [==============================] - 45s 7ms/step - loss: 0.1002 - accuracy: 0.9699\n",
      "Epoch 18/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.1004 - accuracy: 0.9704\n",
      "Epoch 19/20\n",
      "6617/6617 [==============================] - 51s 8ms/step - loss: 0.0948 - accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "6617/6617 [==============================] - 40s 6ms/step - loss: 0.0942 - accuracy: 0.9723\n"
>>>>>>> Stashed changes
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "<tensorflow.python.keras.callbacks.History at 0x7f6f4fe42a30>"
      ]
     },
     "execution_count": 85,
=======
       "<keras.callbacks.History at 0x7f6e99d19ca0>"
      ]
     },
     "execution_count": 24,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
<<<<<<< Updated upstream
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
=======
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
>>>>>>> Stashed changes
    "                    verbose=1,\n",
    "                   ) "
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": 21,
   "id": "4419a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
=======
   "cell_type": "markdown",
   "id": "cad730e8",
   "metadata": {},
   "source": [
    "### Save and Load Models"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 25,
>>>>>>> Stashed changes
   "id": "985551b1",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "# model.save(\"my_model.h5\")"
=======
    "# model.save(\"mlp_model.h5\", save_format='h5')"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 26,
>>>>>>> Stashed changes
   "id": "31e84590",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "# model = keras.models.load_model(save_format='h5', \"my_model.h5\")"
=======
    "# model = keras.models.load_model(\"mlp_model.h5\")"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 38,
   "id": "a6d2ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = os.path.join(POS_DIR, \"test.txt\")\n",
    "\n",
    "test_sentences = format_data(pos_test, False)\n",
    "\n",
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)"
=======
   "execution_count": 27,
   "id": "802fbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim = embeddings.vectors.shape[1]\n",
    "# pad = np.zeros(dim)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 39,
   "id": "8b949bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_correct = os.path.join(POS_DIR, \"test_labelled.txt\") \n",
=======
   "execution_count": 28,
   "id": "feda2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_correct = '/home/chitrang/Downloads/test.txt'\n",
>>>>>>> Stashed changes
    "correct_test_sen = format_data(pos_test_correct)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 40,
=======
   "execution_count": 29,
>>>>>>> Stashed changes
   "id": "3fd0f3de",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
=======
    "# Compare Results against Groundtruth\n",
>>>>>>> Stashed changes
    "def compare_results(predicted_sen, correct_sen, acc = True):\n",
    "    total = 0\n",
    "    correct_total = 0\n",
    "    if acc:\n",
    "        for predict, correct in zip(predicted_sen, correct_sen):\n",
    "                if predict[1] == correct[1]:\n",
    "                    correct_total = correct_total + 1\n",
    "                total = total + 1\n",
    "        acc = correct_total / total\n",
    "        return acc\n",
    "    else:\n",
    "        for predict, correct in zip(predicted_sen, correct_sen):\n",
    "                if predict[1] == correct[1]:\n",
    "                    print(f\"{predict[1]} \\t {correct[1]}\")\n",
    "                else:\n",
    "                    print(f\"{predict[1]} \\t {correct[1]} <------ Error\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 86,
=======
   "execution_count": 30,
>>>>>>> Stashed changes
   "id": "154290dd",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "test_sentences = format_data(pos_test, False)\n",
    "\n",
    "preprocessed_test_data = preprocess_unlabelled_test_data(embeddings, oov, test_sentences, window=WINDOW_SIZE)"
=======
    "preprocessed_test_data = []\n",
    "# Embed test data\n",
    "def embed_test_sentences(sentence):\n",
    "    X_embs = [x[1][0] for x in sentence]\n",
    "    X_embs = np.asarray(X_embs)\n",
    "    return X_embs\n",
    "\n",
    "# Preprocess Unlabelled Data\n",
    "def preprocess_unlabelled_test_data(test_sentences):\n",
    "    for sentence in test_sentences:\n",
    "        sentence = transform_test_sentence(sentence, 2)\n",
    "        embedded = embed_test_sentences(sentence)\n",
    "        preprocessed_test_data.append(embedded)\n",
    "\n",
    "# Generate Labelled Data from predicted data\n",
    "def generate_labelled_data(file_name):\n",
    "    f = open(file_name, \"w\")\n",
    "    for sentence in predicted_data:\n",
    "        for word, pos in sentence:\n",
    "            f.write(f\"{word} {pos}\\n\")\n",
    "        f.write(f\"\\n\")\n",
    "    f.close()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 87,
=======
   "execution_count": 31,
   "id": "8d6ed0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_unlabelled_test_data(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
>>>>>>> Stashed changes
   "id": "1b78dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = []\n",
    "arg_max_dict = []\n",
    "def test_set_predictions(preprocessed_test_data, test_sentences):\n",
    "    for sentence in preprocessed_test_data:\n",
    "        predict_x=model.predict(sentence, batch_size=1, verbose=0) \n",
    "        predict_x = np.argmax(predict_x, axis=1)\n",
    "        arg_max_dict.append(predict_x)\n",
    "        \n",
    "    for index in range(len(test_sentences)):\n",
    "        predicted_sen = list(zip(test_sentences[index], le.inverse_transform(arg_max_dict[index])))\n",
    "        predicted_data.append(predicted_sen)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 88,
=======
   "execution_count": 33,
>>>>>>> Stashed changes
   "id": "5e3da54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions(preprocessed_test_data, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "69d639ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data[:2]"
=======
   "execution_count": 34,
   "id": "5a0ea441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_test_set(correct_set):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for predicted_sentence, correct_sentence in zip(predicted_data, correct_set):\n",
    "        for predicted_word, correct_word in zip(predicted_sentence, correct_sentence):\n",
    "            total = total + 1\n",
    "            if predicted_word[1] == correct_word[1]:\n",
    "                correct = correct + 1\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 89,
=======
   "execution_count": 35,
>>>>>>> Stashed changes
   "id": "dd054b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "96.12892331722144"
      ]
     },
     "execution_count": 89,
=======
       "96.0740443675201"
      ]
     },
     "execution_count": 35,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "compare_with_test_set(predicted_data, correct_test_sen)"
=======
    "compare_with_test_set(correct_test_sen)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "d848310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labelled_data(file_name):\n",
    "    f = open(file_name, \"w\")\n",
    "    for sentence in predicted_data:\n",
    "        for word, pos in sentence:\n",
    "            f.write(f\"{word} {pos}\\n\")\n",
    "        f.write(f\"\\n\")\n",
    "    f.close()"
=======
   "execution_count": 37,
   "id": "c3ff8046",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_labelled_data('./../Labelled_outputs/mlp_labelled_best.txt')"
>>>>>>> Stashed changes
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": "pos",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "26b82d29683d0e2e5d2cbb75bad0064bccae605df6fb9f9bb51c3186d8a354e6"
   }
=======
   "version": "3.8.16"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
