{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f148fc8",
   "metadata": {},
   "source": [
    "# Logistic Regression for POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f664f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "import os, random, datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# automatic module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(3) \n",
    "random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15914b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path for imports to work\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# src imports\n",
    "from src.utils import get_root_dir\n",
    "from src.parser import format_data, embeddings_init\n",
    "from src.data_helpers import vectorize, preprocess_unlabelled_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72117817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset helper functions \n",
    "def dataset_init():\n",
    "    dataset_path = os.path.join(get_root_dir(), 'dataset')\n",
    "    train_data_path = os.path.join(dataset_path, \"train.txt\")\n",
    "    test_data_path = os.path.join(dataset_path, \"test.txt\")\n",
    "    labelled_test_data_path =  os.path.join(dataset_path, \"test_labelled.txt\")\n",
    "    # ==++++++++++++Rewrite+++++++++++==\n",
    "    if not os.path.exists(train_data_path) or not os.path.exists(test_data_path) or not\\\n",
    "            os.path.exists(labelled_test_data_path):\n",
    "        raise FileNotFoundError(\"Check dataset paths!\")\n",
    "    return train_data_path, test_data_path, labelled_test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f41a4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/testing helper functions\n",
    "def test_set_predictions(model, preprocessed_test_data, test_sentences):\n",
    "    arg_max_dict = []\n",
    "    for sentence in preprocessed_test_data:\n",
    "        predict_x = model.predict(sentence)\n",
    "        # predict_x = np.argmax(predict_x, axis=0)\n",
    "        arg_max_dict.append(predict_x)\n",
    "\n",
    "    predicted_data = []\n",
    "    for index in range(len(test_sentences)):\n",
    "        predicted_sen = list(zip(test_sentences[index], arg_max_dict[index]))\n",
    "        predicted_data.append(predicted_sen)\n",
    "\n",
    "    return predicted_data, arg_max_dict\n",
    "\n",
    "def compare_with_test_set(predicted_data, correct_set):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for predicted_sentence, correct_sentence in zip(predicted_data, correct_set):\n",
    "        for predicted_word, correct_word in zip(predicted_sentence, correct_sentence):\n",
    "            total = total + 1\n",
    "            if predicted_word[1] == correct_word[1]:\n",
    "                correct = correct + 1\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a837561",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bc808bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings\n",
      "Initialiation completed\n",
      "Embeddings loaded in 250.512443 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize Embeddings\n",
    "embeddings_path = os.path.join(get_root_dir(), 'dataset')\n",
    "t_ini = datetime.datetime.now()\n",
    "print('Initializing embeddings')\n",
    "embeddings = embeddings_init(str(embeddings_path))\n",
    "print('Initialiation completed')\n",
    "t_fin = datetime.datetime.now()\n",
    "print('Embeddings loaded in {} seconds'.format((t_fin - t_ini).total_seconds()))\n",
    "\n",
    "# Global constants for feature engineering\n",
    "dim = embeddings.vectors.shape[1]\n",
    "pad = np.zeros(dim)  # Pad vector\n",
    "oov = np.random.uniform(-0.25, 0.25, dim)  # Out-of-vocabulary vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2870548b",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c53f35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_training_without_cross_validation(X_train, y_train, model_filename='lr-model1.pkl'):\n",
    "    t_ini = datetime.datetime.now()\n",
    "    print('Training...')\n",
    "    clf = LogisticRegression(C=1, solver='liblinear', multi_class='auto', random_state=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    t_fin = datetime.datetime.now()\n",
    "    print('Training completed in {} seconds'.format((t_fin - t_ini).total_seconds()))\n",
    "\n",
    "    # save model\n",
    "    print('Saving model...')\n",
    "    with open(model_filename, 'wb') as file:\n",
    "        pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5039ba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mdl/afs6372/CSE-582/dataset /home/mdl/afs6372/CSE-582/dataset/train.txt /home/mdl/afs6372/CSE-582/dataset/test.txt /home/mdl/afs6372/CSE-582/dataset/test_labelled.txt\n",
      "True\n",
      "/home/mdl/afs6372/CSE-582/dataset/train.txt\n",
      "Tagged sentences in train set:  8936\n",
      "Tagged words in train set: 211727\n",
      "Initializing vectorization...\n",
      "Embeddings window method\n",
      "Vectorizing Dataset...\n",
      "Vectorizing train...\n",
      "Dataset vectorized.\n",
      "Train shape: (211727, 900)\n",
      "Completed vectorization...\n",
      "Vectorization completed in 18.380086 seconds\n",
      "Training...\n",
      "Training completed in 3919.563265 seconds\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Datasets\n",
    "train_path, test_path, labelled_test_path  = dataset_init()\n",
    "\n",
    "# Preprocessing on training dataset\n",
    "train_sentences = format_data(train_path)\n",
    "#\n",
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))\n",
    "#\n",
    "t_ini = datetime.datetime.now()\n",
    "print('Initializing vectorization...')\n",
    "X_train, y_train = vectorize(embeddings, oov, train_sentences, window=1)\n",
    "print('Completed vectorization...')\n",
    "t_fin = datetime.datetime.now()\n",
    "print('Vectorization completed in {} seconds'.format((t_fin - t_ini).total_seconds()))\n",
    "#\n",
    "init_training_without_cross_validation(X_train, y_train, model_filename='lr-model3.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2cdce16",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "267ffaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filename):\n",
    "    with open(model_filename, 'rb') as file:\n",
    "        Pickled_LR_Model = pickle.load(file)\n",
    "\n",
    "    return Pickled_LR_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5b65298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "clf = load_model(model_filename='lr-model3.pkl')\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "correct_test_sen = format_data(labelled_test_path)\n",
    "test_sentences = format_data(test_path, False)\n",
    "\n",
    "\n",
    "preprocessed_test_data = preprocess_unlabelled_test_data(embeddings, oov, test_sentences)\n",
    "# compute predictions for the test data\n",
    "predicted_data, arg_max_dict = test_set_predictions(clf, preprocessed_test_data, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0c5fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.38757202862148\n",
      "[[('rockwell', 'NNP'), ('international', 'NNP'), ('corp.', 'NNP'), (\"'s\", 'POS'), ('tulsa', 'NNS'), ('unit', 'NNP'), ('said', 'VBD'), ('it', 'PRP'), ('signed', 'VBD'), ('a', 'DT'), ('tentative', 'JJ'), ('agreement', 'NN'), ('extending', 'VBG'), ('its', 'PRP$'), ('contract', 'NN'), ('with', 'IN'), ('boeing', 'NNP'), ('co.', 'NNP'), ('to', 'TO'), ('provide', 'VB'), ('structural', 'JJ'), ('parts', 'NNS'), ('for', 'IN'), ('boeing', 'NNP'), (\"'s\", 'POS'), ('747', 'NN'), ('jetliners', 'NNS'), ('.', '.')], [('rockwell', 'NNP'), ('said', 'VBD'), ('the', 'DT'), ('agreement', 'NN'), ('calls', 'VBZ'), ('for', 'IN'), ('it', 'PRP'), ('to', 'TO'), ('supply', 'VB'), ('200', 'CD'), ('additional', 'JJ'), ('so-called', 'JJ'), ('shipsets', '``'), ('for', 'IN'), ('the', 'DT'), ('planes', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(compare_with_test_set(predicted_data, correct_test_sen))  # Accuracy = 94\n",
    "print(predicted_data[:2])\n",
    "\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(str(predicted_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "26b82d29683d0e2e5d2cbb75bad0064bccae605df6fb9f9bb51c3186d8a354e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
